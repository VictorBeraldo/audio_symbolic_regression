{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "56d37298",
   "metadata": {},
   "source": [
    "# Audio Spoofing Detection Notebook\n",
    "\n",
    "This notebook aims to detect spoofed audio using Symbolic Regression with PySR. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d090f140",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pysr import PySRRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bca8ff5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "# Setup and Initial Imports\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import sys\n",
    "import os\n",
    "import time\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Add the project root folder to PYTHONPATH\n",
    "sys.path.append(os.path.abspath('..'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b776409",
   "metadata": {},
   "source": [
    "## Import Machine Learning and SR Libraries\n",
    "\n",
    "In this cell, we import the necessary libraries for SR and data processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efaa55bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, det_curve\n",
    "from sklearn.model_selection import StratifiedShuffleSplit"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccc8b2d1",
   "metadata": {},
   "source": [
    "## Define File Paths (Validation)\n",
    "\n",
    "Define the paths for the processed features and protocol file. Make sure to adjust these paths as needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57169be4",
   "metadata": {},
   "outputs": [],
   "source": [
    "TEST_FEATURES_MFCC  = \"../data/processed/mfccs_features_ASVspoof2021_DF_eval_part00.pkl\"\n",
    "TRAIN_FEATURES_MFCC = \"../data/processed/mfccs_features.pkl\"\n",
    "TEST_PROTOCOL  = \"../data/raw/DF-keys-full/keys/DF/CM/trial_metadata.txt\"\n",
    "TRAIN_PROTOCOL = \"../data/raw/LA/ASVspoof2019_LA_cm_protocols/ASVspoof2019.LA.cm.train.trn.txt\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be23f74d",
   "metadata": {},
   "source": [
    "## Load Features\n",
    "\n",
    "Load the feature data from the pickle file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b15537a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(TRAIN_FEATURES_MFCC, 'rb') as f:\n",
    "    train_data = pickle.load(f)\n",
    "with open(TEST_FEATURES_MFCC, 'rb') as f:\n",
    "    test_data = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd2e32c3",
   "metadata": {},
   "source": [
    "## Read Protocol File\n",
    "\n",
    "Define a function to read the protocol file, which maps audio files to labels, and load the DataFrame for train and test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "884f2844",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Protocol Data (first 5 rows):\n",
      "  speaker_id    audio_file system_id       key\n",
      "0    LA_0079  LA_T_1138215         -  bonafide\n",
      "1    LA_0079  LA_T_1271820         -  bonafide\n",
      "2    LA_0079  LA_T_1272637         -  bonafide\n",
      "3    LA_0079  LA_T_1276960         -  bonafide\n",
      "4    LA_0079  LA_T_1341447         -  bonafide\n"
     ]
    }
   ],
   "source": [
    "def read_cm_protocol_train(filepath: str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Reads the ASVspoof2019 LA train protocol file and returns a DataFrame with:\n",
    "      - speaker_id : speaker identifier (e.g. LA_0079)\n",
    "      - audio_file : trial identifier (e.g. LA_T_1138215)\n",
    "      - system_id  : system placeholder ('-')\n",
    "      - key        : bonafide/spoof label\n",
    "\n",
    "    Args:\n",
    "        filepath (str): path to the ASVspoof2019.LA.cm.train.trn.txt file.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: columns = [speaker_id, audio_file, system_id, key]\n",
    "    \"\"\"\n",
    "    # the file has 5 whitespace-separated fields, the 4th is unused ('-')\n",
    "    col_names = [\"speaker_id\", \"audio_file\", \"system_id\", \"unused\", \"key\"]\n",
    "    df = pd.read_csv(\n",
    "        filepath,\n",
    "        sep=r\"\\s+\",\n",
    "        names=col_names,\n",
    "        engine=\"python\",\n",
    "        index_col=False\n",
    "    )\n",
    "    df = df.drop(columns=[\"unused\"])\n",
    "    return df\n",
    "\n",
    "# Usage example:\n",
    "train_df = read_cm_protocol_train(TRAIN_PROTOCOL)\n",
    "print(\"Train Protocol (first 5 rows):\")\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccca14a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_cm_protocol_test(filepath: str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Reads a CM protocol file and returns a DataFrame with the following columns:\n",
    "      - speaker_id     : speaker identifier (e.g., LA_0023)\n",
    "      - trial_id       : trial identifier (e.g., DF_E_2000011)\n",
    "      - codec          : compression codec (e.g., nocodec, low_mp3, high_ogg, etc.)\n",
    "      - data_source    : data origin (asvspoof, vcc2018, vcc2020)\n",
    "      - attack_id      : spoofing attack code (A07–A19)\n",
    "      - key            : trial label (bonafide or spoof)\n",
    "      - trim           : speech trimming flag (notrim or trim)\n",
    "      - subset         : subset name (eval, progress, hidden)\n",
    "      - vocoder_type   : vocoder category\n",
    "    The remaining four fields (always “-”) are discarded.\n",
    "    \"\"\"\n",
    "    # Define all 13 column names but only keep the first 9\n",
    "    column_names = [\n",
    "        \"speaker_id\", \"trial_id\", \"codec\", \"data_source\", \"attack_id\",\n",
    "        \"key\", \"trim\", \"subset\", \"vocoder_type\",\n",
    "        \"unused1\", \"unused2\", \"unused3\", \"unused4\"\n",
    "    ]\n",
    "    \n",
    "    # Read using any amount of whitespace as delimiter\n",
    "    df = pd.read_csv(\n",
    "        filepath,\n",
    "        sep=r\"\\s+\",\n",
    "        names=column_names,\n",
    "        engine=\"python\",\n",
    "        index_col=False\n",
    "    )\n",
    "    \n",
    "    # Drop the unused placeholder columns\n",
    "    df = df.drop(columns=[\"unused1\", \"unused2\", \"unused3\", \"unused4\"])\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Example usage:\n",
    "test_df = read_cm_protocol_test(TEST_PROTOCOL)\n",
    "test_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49010eb1",
   "metadata": {},
   "source": [
    "## Create DataFrame for Features and Labels\n",
    "\n",
    "Create a DataFrame where each row contains a file name and its corresponding features. Label 1 represents \"spoof\" and 0 represents \"bonafide\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e47ce5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_df.shape, test_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00a3f810",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_labeled_df(data: list, protocol_df: pd.DataFrame, id_col: str) -> pd.DataFrame:\n",
    "    # map trial ID → label\n",
    "    label_map = dict(zip(protocol_df[id_col], protocol_df['key']))\n",
    "    # build raw features DataFrame\n",
    "    n_feats = len(data[0]['features'])\n",
    "    cols = ['file'] + [f'feature_{i}' for i in range(n_feats)]\n",
    "    df = pd.DataFrame([ (d['file'], *d['features']) for d in data ], columns=cols)\n",
    "    # strip extension and map to 0/1\n",
    "    df['trial_id'] = df['file'].str.replace(r'\\.(wav|flac)$','',regex=True)\n",
    "    df['label'] = (df['trial_id']\n",
    "                   .map(label_map)\n",
    "                   .fillna('bonafide')\n",
    "                   .map({'spoof':1,'bonafide':0}))\n",
    "    return df.drop(columns=['trial_id'])\n",
    "\n",
    "# -- Create feature+label DataFrames\n",
    "train_features_df = create_labeled_df(train_data, train_df, 'audio_file')\n",
    "del(train_data)\n",
    "test_features_df  = create_labeled_df(test_data,  test_df,  'trial_id')\n",
    "del(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b842f293",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -- Inspect\n",
    "print(f\"train_features_df: {train_features_df.shape}\")\n",
    "print(train_features_df.head())\n",
    "print(f\"test_features_df: {test_features_df.shape}\")\n",
    "print(test_features_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "480086cd",
   "metadata": {},
   "source": [
    "## Data Exploration\n",
    "\n",
    "Display the distribution of labels to verify the balance of the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a299697",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label Distribution:\n",
      "          Count  Percent (%)\n",
      "label                       \n",
      "spoof     22800        89.83\n",
      "bonafide   2580        10.17\n"
     ]
    }
   ],
   "source": [
    "def summarize_label_distribution(df: pd.DataFrame, name: str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Returns a summary DataFrame with counts and percentages of bona-fide vs spoof labels.\n",
    "    \n",
    "    Args:\n",
    "        df   : DataFrame containing a 'label' column (0 = bonafide, 1 = spoof)\n",
    "        name : identifier for printing (e.g., \"Train\", \"Test\")\n",
    "    \"\"\"\n",
    "    # map 0/1 → string labels\n",
    "    label_map = {0: \"bonafide\", 1: \"spoof\"}\n",
    "    \n",
    "    # count and percentage\n",
    "    dist = df[\"label\"].value_counts().rename(index=label_map)\n",
    "    pct  = df[\"label\"].value_counts(normalize=True).mul(100).rename(index=label_map).round(2)\n",
    "    \n",
    "    summary = pd.DataFrame({\n",
    "        \"Count\":       dist,\n",
    "        \"Percent (%)\": pct\n",
    "    })\n",
    "    \n",
    "    print(f\"\\n{name} set label distribution:\")\n",
    "    print(summary)\n",
    "    return summary\n",
    "\n",
    "# Usage for train and test sets\n",
    "train_summary = summarize_label_distribution(train_features_df, \"Train\")\n",
    "test_summary  = summarize_label_distribution(test_features_df,  \"Test\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea354463",
   "metadata": {},
   "source": [
    "## Data Preprocessing\n",
    "\n",
    "Separate the features (X) and labels (y), standardize the features, and split the data into training and testing sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69853a6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare feature matrices and label vectors from pre-defined train/test DataFrames\n",
    "X_train = train_features_df.drop(columns=[\"file\", \"label\"]).values\n",
    "y_train = train_features_df[\"label\"].values\n",
    "\n",
    "X_test  = test_features_df.drop(columns=[\"file\", \"label\"]).values\n",
    "y_test  = test_features_df[\"label\"].values\n",
    "\n",
    "# Fit scaler on train only, then transform both\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test  = scaler.transform(X_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5388b2ee",
   "metadata": {},
   "source": [
    "## Define, Train, and Evaluate Models\n",
    "\n",
    "Define a dictionary of models, then train and evaluate each one. For each model, we calculate the training time, accuracy, confusion matrix, and Equal Error Rate (EER)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61324919",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/vicosbe/victo/audio_symbolic_regression/audio_symbolic_regression_env/bin/python\n"
     ]
    }
   ],
   "source": [
    "SAMPLE_FRAC         = 1    # keep % of each set\n",
    "SAMPLE_RANDOM_STATE = 42     # reproducible subsampling\n",
    "\n",
    "SEEDS = range(1, 11)          # seeds 1…20\n",
    "TICKS = [0.1, 0.2, 0.5, 1, 2, 5, 10, 20, 50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "902e98c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def stratified_subsample(X, y, frac, random_state):\n",
    "    \"\"\"Return a stratified subsample of (X, y).\"\"\"\n",
    "    if frac >= 1.0:\n",
    "        return X, y\n",
    "    sss = StratifiedShuffleSplit(n_splits=1, train_size=frac, random_state=random_state)\n",
    "    idx, _ = next(sss.split(X, y))\n",
    "    return X[idx], y[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48684d97",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Subsample both train and test sets\n",
    "X_train_sub, y_train_sub = stratified_subsample(X_train, y_train, SAMPLE_FRAC, SAMPLE_RANDOM_STATE)\n",
    "X_test_sub,  y_test_sub  = stratified_subsample(X_test,  y_test,  SAMPLE_FRAC, SAMPLE_RANDOM_STATE)\n",
    "\n",
    "print(f\"Subsampled train: {len(y_train_sub)}/{len(y_train)} samples\")\n",
    "print(f\"Subsampled test : {len(y_test_sub)}/{len(y_test)} samples\\n\")\n",
    "\n",
    "# Training & evaluation loop on subsampled data\n",
    "print(\"Training and evaluating models on subsampled data...\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9735c4f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_nist_det(fpr_tr, fnr_tr, fpr_ts, fnr_ts, title):\n",
    "    # convert to percent\n",
    "    fpr_tr_p = fpr_tr * 100\n",
    "    fnr_tr_p = fnr_tr * 100\n",
    "    fpr_ts_p = fpr_ts * 100\n",
    "    fnr_ts_p = fnr_ts * 100\n",
    "\n",
    "    ticks = [0.1, 0.2, 0.5, 1, 2, 5, 10, 20, 50]\n",
    "\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.plot(fpr_tr_p, fnr_tr_p, label=\"Train DET\", linewidth=1.5)\n",
    "    ax.plot(fpr_ts_p, fnr_ts_p, label=\"Test DET\",  linewidth=1.5)\n",
    "\n",
    "    ax.set_xscale(\"log\")\n",
    "    ax.set_yscale(\"log\")\n",
    "    ax.set_xlim(min(ticks), max(ticks))\n",
    "    ax.set_ylim(min(ticks), max(ticks))\n",
    "    ax.set_xticks(ticks)\n",
    "    ax.set_yticks(ticks)\n",
    "    ax.xaxis.set_major_formatter(ticker.ScalarFormatter())\n",
    "    ax.yaxis.set_major_formatter(ticker.ScalarFormatter())\n",
    "\n",
    "    # diagonal EER line\n",
    "    ax.plot(ticks, ticks, linestyle=\"--\", color=\"grey\", linewidth=1)\n",
    "\n",
    "    ax.set_xlabel(\"False Acceptance Rate (%)\")\n",
    "    ax.set_ylabel(\"False Rejection Rate (%)\")\n",
    "    ax.set_title(title)\n",
    "    ax.grid(which=\"both\", linestyle=\"--\", linewidth=0.5)\n",
    "    ax.legend()\n",
    "    plt.tight_layout()\n",
    "    return fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df4414cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------------------------------------------\n",
    "# PySR model templates\n",
    "# -------------------------------------------------------------------\n",
    "model_templates = {\n",
    "    \"PySR_batch_500_200it (Basic)\": lambda seed: PySRRegressor(\n",
    "        niterations=2,\n",
    "        binary_operators=[\"+\", \"-\", \"*\", \"/\"],\n",
    "        unary_operators=[\"exp\", \"log\"],\n",
    "        elementwise_loss=\"(x, y) -> (x - y)^2\",\n",
    "        random_state=seed,\n",
    "        batching=True,\n",
    "        batch_size=500,\n",
    "        deterministic=True,\n",
    "        parallelism=\"serial\",\n",
    "        constraints={ \"^\": (-1, 1) }\n",
    "    ),\n",
    "    \"PySR_batch_500_250it (Extended)\": lambda seed: PySRRegressor(\n",
    "        niterations=2,\n",
    "        binary_operators=[\"+\", \"-\", \"*\", \"/\"],\n",
    "        unary_operators=[\"exp\", \"log\", \"sin\", \"cos\"],\n",
    "        elementwise_loss=\"(x, y) -> (x - y)^2\",\n",
    "        random_state=seed,\n",
    "        batching=True,\n",
    "        batch_size=500,\n",
    "        deterministic=True,\n",
    "        parallelism=\"serial\",\n",
    "        constraints={ \"^\": (-1, 1) }\n",
    "    ),\n",
    "    \"PySR_batch_500_300it (Complex)\": lambda seed: PySRRegressor(\n",
    "        niterations=3,\n",
    "        binary_operators=[\"+\", \"-\", \"*\", \"/\", \"^\"],\n",
    "        unary_operators=[\"exp\", \"log\", \"sin\", \"cos\", \"abs\"],\n",
    "        elementwise_loss=\"(x, y) -> (x - y)^2\",\n",
    "        random_state=seed,\n",
    "        batching=True,\n",
    "        batch_size=500,\n",
    "        deterministic=True,\n",
    "        parallelism=\"serial\",\n",
    "        constraints={ \"^\": (-1, 1) }\n",
    "    ),\n",
    "}\n",
    "\n",
    "# -------------------------------------------------------------------\n",
    "# Prepare metrics CSV (add EER_train column)\n",
    "# -------------------------------------------------------------------\n",
    "csv_file = \"model_metrics_test_mfcc.csv\"\n",
    "if not os.path.exists(csv_file):\n",
    "    pd.DataFrame(\n",
    "        columns=[\"Model\", \"Training Time (sec)\", \"Accuracy\", \"EER_train\", \"EER\"]\n",
    "    ).to_csv(csv_file, index=False)\n",
    "\n",
    "# -------------------------------------------------------------------\n",
    "# Loop over templates and seeds\n",
    "# -------------------------------------------------------------------\n",
    "SEEDS    = range(1, 11)\n",
    "BASE_DIR = \"../models/test/py_sr_mfcc\"\n",
    "os.makedirs(BASE_DIR, exist_ok=True)\n",
    "\n",
    "print(\"\\nTraining PySR models with multiple seeds…\\n\")\n",
    "for name, constructor in model_templates.items():\n",
    "    print(f\"=== {name} ===\")\n",
    "    safe_name = name.replace(\" \", \"_\").replace(\"(\", \"\").replace(\")\", \"\")\n",
    "    model_dir = os.path.join(BASE_DIR, safe_name)\n",
    "    os.makedirs(model_dir, exist_ok=True)\n",
    "\n",
    "    for seed in SEEDS:\n",
    "        model = constructor(seed)\n",
    "\n",
    "        # Train\n",
    "        t0 = time.time()\n",
    "        model.fit(X_train_sub, y_train_sub)\n",
    "        train_time = time.time() - t0\n",
    "\n",
    "        # TRAIN DET (sanitize + compute EER_train)\n",
    "        y_scores_tr = np.array(model.predict(X_train_sub), dtype=float)\n",
    "        mask_tr     = np.isfinite(y_scores_tr)\n",
    "        fpr_tr, fnr_tr, _ = det_curve(y_train_sub[mask_tr], y_scores_tr[mask_tr])\n",
    "        idx_tr     = np.nanargmin(np.abs(fnr_tr - fpr_tr))\n",
    "        eer_train  = float(fpr_tr[idx_tr])\n",
    "\n",
    "        # TEST DET (sanitize + compute EER)\n",
    "        y_scores_ts = np.array(model.predict(X_test_sub), dtype=float)\n",
    "        mask_ts     = np.isfinite(y_scores_ts)\n",
    "        y_true_ts   = y_test_sub[mask_ts]\n",
    "        y_scores_ts = y_scores_ts[mask_ts]\n",
    "        fpr_ts, fnr_ts, _ = det_curve(y_true_ts, y_scores_ts)\n",
    "        idx_ts      = np.nanargmin(np.abs(fnr_ts - fpr_ts))\n",
    "        eer         = float(fpr_ts[idx_ts])\n",
    "        y_pred_ts   = (y_scores_ts > 0.5).astype(int)\n",
    "        acc         = accuracy_score(y_true_ts, y_pred_ts)\n",
    "\n",
    "        # Save model\n",
    "        with open(os.path.join(model_dir, f\"model_seed_{seed:02d}.pkl\"), \"wb\") as f:\n",
    "            pickle.dump(model, f)\n",
    "\n",
    "        # Plot & save DET\n",
    "        fig = plot_nist_det(fpr_tr, fnr_tr, fpr_ts, fnr_ts, title=f\"{name} – Seed {seed:02d}\")\n",
    "        fig.savefig(os.path.join(model_dir, f\"DET_seed_{seed:02d}.png\"))\n",
    "        plt.close(fig)\n",
    "\n",
    "        # Append metrics (including EER_train)\n",
    "        pd.DataFrame([{\n",
    "            \"Model\": name,\n",
    "            \"Training Time (sec)\": round(train_time, 2),\n",
    "            \"Accuracy\": round(acc, 4),\n",
    "            \"EER_train\": round(eer_train, 4),\n",
    "            \"EER\": round(eer, 4)\n",
    "        }]).to_csv(csv_file, mode=\"a\", header=False, index=False)\n",
    "\n",
    "        print(f\"Seed {seed:02d}: time {train_time:.1f}s, acc {acc:.4f}, \"\n",
    "              f\"EER_train {eer_train:.4f}, EER_test {eer:.4f}\")\n",
    "    print()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "audio_symbolic_regression_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
