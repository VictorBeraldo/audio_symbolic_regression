{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "56d37298",
   "metadata": {},
   "source": [
    "# Audio Spoofing Detection Notebook\n",
    "\n",
    "This notebook aims to detect spoofed audio using Symbolic Regression with PySR. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8bca8ff5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "# Setup and Initial Imports\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import sys\n",
    "import os\n",
    "import time\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Add the project root folder to PYTHONPATH\n",
    "sys.path.append(os.path.abspath('..'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b776409",
   "metadata": {},
   "source": [
    "## Import Machine Learning and SR Libraries\n",
    "\n",
    "In this cell, we import the necessary libraries for SR and data processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "efaa55bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, det_curve\n",
    "from lightgbm import LGBMClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.svm import SVC"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccc8b2d1",
   "metadata": {},
   "source": [
    "## Define File Paths\n",
    "\n",
    "Define the paths for the processed features and protocol file. Make sure to adjust these paths as needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "57169be4",
   "metadata": {},
   "outputs": [],
   "source": [
    "features_path = '../data/processed/filterbanks_features.pkl'\n",
    "protocol_path = '../data/raw/LA/ASVspoof2019_LA_cm_protocols/ASVspoof2019.LA.cm.train.trn.txt'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be23f74d",
   "metadata": {},
   "source": [
    "## Load Features\n",
    "\n",
    "Load the feature data from the pickle file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b15537a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(features_path, 'rb') as f:\n",
    "    data = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "42ddac20",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(640,)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[0]['features'].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd2e32c3",
   "metadata": {},
   "source": [
    "## Read Protocol File\n",
    "\n",
    "Define a function to read the protocol file, which maps audio files to labels, and load the DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "884f2844",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Protocol Data (first 5 rows):\n",
      "  speaker_id    audio_file system_id       key\n",
      "0    LA_0079  LA_T_1138215         -  bonafide\n",
      "1    LA_0079  LA_T_1271820         -  bonafide\n",
      "2    LA_0079  LA_T_1272637         -  bonafide\n",
      "3    LA_0079  LA_T_1276960         -  bonafide\n",
      "4    LA_0079  LA_T_1341447         -  bonafide\n"
     ]
    }
   ],
   "source": [
    "def read_cm_protocol(filepath: str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Reads the CM protocol file and returns a DataFrame.\n",
    "    \n",
    "    Args:\n",
    "        filepath (str): Path to the protocol file.\n",
    "    \n",
    "    Returns:\n",
    "        pd.DataFrame: DataFrame with speaker_id, audio_file, system_id, and key.\n",
    "    \"\"\"\n",
    "    column_names = [\"speaker_id\", \"audio_file\", \"system_id\", \"unused\", \"key\"]\n",
    "    df = pd.read_csv(filepath, sep=' ', names=column_names, index_col=False)\n",
    "    df.drop(columns=[\"unused\"], inplace=True)\n",
    "    return df\n",
    "\n",
    "protocol_df = read_cm_protocol(protocol_path)\n",
    "print(\"Protocol Data (first 5 rows):\")\n",
    "print(protocol_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49010eb1",
   "metadata": {},
   "source": [
    "## Create DataFrame for Features and Labels\n",
    "\n",
    "Create a DataFrame where each row contains a file name and its corresponding features. Label 1 represents \"spoof\" and 0 represents \"bonafide\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7b5c266f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features DataFrame (first 5 rows):\n",
      "                file  feature_0  feature_1  feature_2  feature_3  feature_4  \\\n",
      "0  LA_T_1000137.flac   0.000010   0.000022   0.000072   0.001275   0.046314   \n",
      "1  LA_T_1000406.flac   0.000011   0.000016   0.000036   0.000889   0.011094   \n",
      "2  LA_T_1000648.flac   0.000003   0.000003   0.000006   0.000099   0.001488   \n",
      "3  LA_T_1000824.flac   0.000017   0.000050   0.000262   0.011847   0.271454   \n",
      "4  LA_T_1001074.flac   0.000002   0.000004   0.000015   0.000821   0.015475   \n",
      "\n",
      "   feature_5  feature_6  feature_7  feature_8  ...  feature_631  feature_632  \\\n",
      "0   0.565488   2.776286   6.029241  10.804525  ...     0.000097     0.000081   \n",
      "1   0.033539   0.144185   1.887749   7.177183  ...     0.000002     0.000002   \n",
      "2   0.003842   0.004142   0.003567   0.004978  ...     0.000042     0.000038   \n",
      "3   2.040871   4.335693   3.998420   1.990429  ...     0.000004     0.000006   \n",
      "4   0.060690   0.101386   0.129564   0.271895  ...     0.000003     0.000002   \n",
      "\n",
      "   feature_633   feature_634  feature_635  feature_636  feature_637  \\\n",
      "0     0.000044  3.967310e-05     0.000051     0.000037     0.000033   \n",
      "1     0.000002  1.535568e-06     0.000002     0.000002     0.000001   \n",
      "2     0.000020  5.897118e-06     0.000006     0.000004     0.000003   \n",
      "3     0.000010  2.295864e-05     0.000042     0.000092     0.000093   \n",
      "4     0.000001  9.699645e-07     0.000001     0.000002     0.000002   \n",
      "\n",
      "    feature_638   feature_639  label  \n",
      "0  8.436957e-06  4.377614e-07      1  \n",
      "1  6.109989e-07  4.428328e-07      0  \n",
      "2  8.213409e-07  1.072867e-07      1  \n",
      "3  2.866999e-05  8.993841e-07      1  \n",
      "4  2.712466e-06  2.846582e-06      1  \n",
      "\n",
      "[5 rows x 642 columns]\n"
     ]
    }
   ],
   "source": [
    "label_dict = dict(zip(protocol_df['audio_file'], protocol_df['key']))\n",
    "df_features = pd.DataFrame(\n",
    "    [(d['file'], *d['features']) for d in data],\n",
    "    columns=[\"file\"] + [f\"feature_{i}\" for i in range(len(data[0]['features']))]\n",
    ")\n",
    "\n",
    "df_features[\"label\"] = df_features[\"file\"].apply(\n",
    "    lambda x: 1 if label_dict.get(x.replace('.flac', '').replace('.wav', ''), 'bonafide') == 'spoof' else 0\n",
    ")\n",
    "\n",
    "print(\"Features DataFrame (first 5 rows):\")\n",
    "print(df_features.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "480086cd",
   "metadata": {},
   "source": [
    "## Data Exploration\n",
    "\n",
    "Display the distribution of labels to verify the balance of the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9a299697",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label Distribution:\n",
      "          Count  Percent (%)\n",
      "label                       \n",
      "spoof     22800        89.83\n",
      "bonafide   2580        10.17\n"
     ]
    }
   ],
   "source": [
    "label_dist = df_features['label'].value_counts().rename(index={0: 'bonafide', 1: 'spoof'})\n",
    "label_pct = df_features['label'].value_counts(normalize=True).mul(100).rename(index={0: 'bonafide', 1: 'spoof'}).round(2)\n",
    "df_label_summary = pd.DataFrame({'Count': label_dist, 'Percent (%)': label_pct})\n",
    "print(\"Label Distribution:\")\n",
    "print(df_label_summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea354463",
   "metadata": {},
   "source": [
    "## Data Preprocessing\n",
    "\n",
    "Separate the features (X) and labels (y), standardize the features, and split the data into training and testing sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "69853a6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df_features.drop(columns=[\"file\", \"label\"]).values\n",
    "y = df_features[\"label\"].values\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X = scaler.fit_transform(X)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7d86fa8",
   "metadata": {},
   "source": [
    "## Dataset Statistics\n",
    "\n",
    "Define a function to display the label distribution in the training and testing sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "63d0d7d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Dataset distribution after split:\n",
      "          Train Count  Train %  Test Count  Test %\n",
      "spoof           18233     89.8        4567   89.97\n",
      "bonafide         2071     10.2         509   10.03\n"
     ]
    }
   ],
   "source": [
    "def dataset_stats(y_train: np.ndarray, y_test: np.ndarray) -> None:\n",
    "    \"\"\"\n",
    "    Prints the distribution of labels for training and testing sets.\n",
    "    \n",
    "    Args:\n",
    "        y_train (np.ndarray): Array of training labels.\n",
    "        y_test (np.ndarray): Array of testing labels.\n",
    "    \"\"\"\n",
    "    train_dist = pd.Series(y_train).value_counts().rename(index={0: 'bonafide', 1: 'spoof'})\n",
    "    test_dist = pd.Series(y_test).value_counts().rename(index={0: 'bonafide', 1: 'spoof'})\n",
    "    train_pct = pd.Series(y_train).value_counts(normalize=True).mul(100).rename(index={0: 'bonafide', 1: 'spoof'}).round(2)\n",
    "    test_pct = pd.Series(y_test).value_counts(normalize=True).mul(100).rename(index={0: 'bonafide', 1: 'spoof'}).round(2)\n",
    "    \n",
    "    df_stats = pd.DataFrame({\n",
    "        'Train Count': train_dist, 'Train %': train_pct,\n",
    "        'Test Count': test_dist, 'Test %': test_pct\n",
    "    })\n",
    "    print(\"\\nDataset distribution after split:\")\n",
    "    print(df_stats)\n",
    "\n",
    "dataset_stats(y_train, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5388b2ee",
   "metadata": {},
   "source": [
    "## Define, Train, and Evaluate Models\n",
    "\n",
    "Define a dictionary of models, then train and evaluate each one. For each model, we calculate the training time, accuracy, confusion matrix, and Equal Error Rate (EER)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "61324919",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/vicosbe/victo/audio_symbolic_regression/audio_symbolic_regression_env/bin/python\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "print(sys.executable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88fb7f10",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pysr import PySRRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ae86491",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definir modelos\n",
    "models = {\n",
    "    \"PySR (Basic)\": PySRRegressor(niterations=80, binary_operators=[\"+\", \"-\", \"*\", \"/\"], unary_operators=[\"exp\", \"log\"], loss=\"loss(x, y) = (x - y)^2\", random_state=42),\n",
    "    \"PySR (Extended)\": PySRRegressor(niterations=100, binary_operators=[\"+\", \"-\", \"*\", \"/\"], unary_operators=[\"exp\", \"log\", \"sin\", \"cos\"], loss=\"loss(x, y) = (x - y)^2\", random_state=42),\n",
    "    \"PySR (Complex)\": PySRRegressor(niterations=200, binary_operators=[\"+\", \"-\", \"*\", \"/\", \"pow\"], unary_operators=[\"exp\", \"log\", \"sin\", \"cos\", \"abs\"], loss=\"loss(x, y) = (x - y)^2\", random_state=42)\n",
    "}\n",
    "\n",
    "# CSV para salvar métricas\n",
    "csv_file = \"model_metrics.csv\"\n",
    "\n",
    "if not os.path.exists(csv_file):\n",
    "    pd.DataFrame(columns=[\"Model\", \"Training Time (sec)\", \"Accuracy\", \"EER\"]).to_csv(csv_file, index=False)\n",
    "\n",
    "print(\"\\nTreinando modelos PySR...\")\n",
    "\n",
    "for name, model in tqdm(models.items(), desc=\"Models\"):\n",
    "    print(f\"\\n{name}:\")\n",
    "    start_time = time.time()\n",
    "\n",
    "    model.fit(X_train, y_train)\n",
    "    train_time = time.time() - start_time\n",
    "\n",
    "    y_pred_proba = model.predict(X_test)\n",
    "    y_pred = (y_pred_proba > 0.5).astype(int)\n",
    "\n",
    "    fpr, fnr, thresholds = det_curve(y_test, y_pred_proba)\n",
    "    idx = np.nanargmin(np.absolute(fnr - fpr))\n",
    "    eer = fpr[idx]\n",
    "    acc = accuracy_score(y_test, y_pred)\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "    print(f\"Tempo de treinamento: {train_time:.2f} segundos\")\n",
    "    print(f\"Acurácia: {acc:.4f}\")\n",
    "    print(\"Matriz de Confusão:\\n\", cm)\n",
    "    print(f\"EER: {eer:.4f}\")\n",
    "\n",
    "    model_filename = f\"{name.replace(' ', '_').replace('(', '').replace(')', '')}.pkl\"\n",
    "    with open(model_filename, \"wb\") as f:\n",
    "        pickle.dump(model, f)\n",
    "    print(f\"Modelo salvo em {model_filename}\")\n",
    "\n",
    "    metrics = {\"Model\": name, \"Training Time (sec)\": round(train_time, 2), \"Accuracy\": round(acc, 4), \"EER\": round(eer, 4)}\n",
    "    pd.DataFrame([metrics]).to_csv(csv_file, mode=\"a\", header=False, index=False)\n",
    "    print(f\"Métricas para {name} adicionadas ao arquivo {csv_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7edf19a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select 10 features\n",
    "\n",
    "# Definir modelos\n",
    "models = {\n",
    "    \"PySR_K10 (Basic)\": PySRRegressor(niterations=40, binary_operators=[\"+\", \"-\", \"*\", \"/\"], unary_operators=[\"exp\", \"log\"], loss=\"loss(x, y) = (x - y)^2\", random_state=42,select_k_features=10),\n",
    "    \"PySR_K10 (Extended)\": PySRRegressor(niterations=60, binary_operators=[\"+\", \"-\", \"*\", \"/\"], unary_operators=[\"exp\", \"log\", \"sin\", \"cos\"], loss=\"loss(x, y) = (x - y)^2\", random_state=42,select_k_features=10),\n",
    "    \"PySR_K10 (Complex)\": PySRRegressor(niterations=80, binary_operators=[\"+\", \"-\", \"*\", \"/\", \"pow\"], unary_operators=[\"exp\", \"log\", \"sin\", \"cos\", \"abs\"], loss=\"loss(x, y) = (x - y)^2\", random_state=42,select_k_features=10)\n",
    "}\n",
    "\n",
    "# CSV para salvar métricas\n",
    "csv_file = \"model_metrics.csv\"\n",
    "\n",
    "if not os.path.exists(csv_file):\n",
    "    pd.DataFrame(columns=[\"Model\", \"Training Time (sec)\", \"Accuracy\", \"EER\"]).to_csv(csv_file, index=False)\n",
    "\n",
    "print(\"\\nTreinando modelos PySR...\")\n",
    "\n",
    "for name, model in tqdm(models.items(), desc=\"Models\"):\n",
    "    print(f\"\\n{name}:\")\n",
    "    start_time = time.time()\n",
    "\n",
    "    model.fit(X_train, y_train)\n",
    "    train_time = time.time() - start_time\n",
    "\n",
    "    y_pred_proba = model.predict(X_test)\n",
    "    y_pred = (y_pred_proba > 0.5).astype(int)\n",
    "\n",
    "    fpr, fnr, thresholds = det_curve(y_test, y_pred_proba)\n",
    "    idx = np.nanargmin(np.absolute(fnr - fpr))\n",
    "    eer = fpr[idx]\n",
    "    acc = accuracy_score(y_test, y_pred)\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "    print(f\"Tempo de treinamento: {train_time:.2f} segundos\")\n",
    "    print(f\"Acurácia: {acc:.4f}\")\n",
    "    print(\"Matriz de Confusão:\\n\", cm)\n",
    "    print(f\"EER: {eer:.4f}\")\n",
    "\n",
    "    model_filename = f\"{name.replace(' ', '_').replace('(', '').replace(')', '')}.pkl\"\n",
    "    with open(model_filename, \"wb\") as f:\n",
    "        pickle.dump(model, f)\n",
    "    print(f\"Modelo salvo em {model_filename}\")\n",
    "\n",
    "    metrics = {\"Model\": name, \"Training Time (sec)\": round(train_time, 2), \"Accuracy\": round(acc, 4), \"EER\": round(eer, 4)}\n",
    "    pd.DataFrame([metrics]).to_csv(csv_file, mode=\"a\", header=False, index=False)\n",
    "    print(f\"Métricas para {name} adicionadas ao arquivo {csv_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21af37c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Batch\n",
    "\n",
    "# Definir modelos\n",
    "models = {\n",
    "    \"PySR_batch (Basic)\": PySRRegressor(niterations=40, binary_operators=[\"+\", \"-\", \"*\", \"/\"], unary_operators=[\"exp\", \"log\"], loss=\"loss(x, y) = (x - y)^2\", random_state=42, batching=True, , batch_size=200),\n",
    "    \"PySR_batch (Extended)\": PySRRegressor(niterations=60, binary_operators=[\"+\", \"-\", \"*\", \"/\"], unary_operators=[\"exp\", \"log\", \"sin\", \"cos\"], loss=\"loss(x, y) = (x - y)^2\", random_state=42, batching=True, batch_size=200),\n",
    "    \"PySR_batch (Complex)\": PySRRegressor(niterations=80, binary_operators=[\"+\", \"-\", \"*\", \"/\", \"pow\"], unary_operators=[\"exp\", \"log\", \"sin\", \"cos\", \"abs\"], loss=\"loss(x, y) = (x - y)^2\", random_state=42, batching=True, batch_size=200)\n",
    "}\n",
    "\n",
    "# CSV para salvar métricas\n",
    "csv_file = \"model_metrics.csv\"\n",
    "\n",
    "if not os.path.exists(csv_file):\n",
    "    pd.DataFrame(columns=[\"Model\", \"Training Time (sec)\", \"Accuracy\", \"EER\"]).to_csv(csv_file, index=False)\n",
    "\n",
    "print(\"\\nTreinando modelos PySR...\")\n",
    "\n",
    "for name, model in tqdm(models.items(), desc=\"Models\"):\n",
    "    print(f\"\\n{name}:\")\n",
    "    start_time = time.time()\n",
    "\n",
    "    model.fit(X_train, y_train)\n",
    "    train_time = time.time() - start_time\n",
    "\n",
    "    y_pred_proba = model.predict(X_test)\n",
    "    y_pred = (y_pred_proba > 0.5).astype(int)\n",
    "\n",
    "    fpr, fnr, thresholds = det_curve(y_test, y_pred_proba)\n",
    "    idx = np.nanargmin(np.absolute(fnr - fpr))\n",
    "    eer = fpr[idx]\n",
    "    acc = accuracy_score(y_test, y_pred)\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "    print(f\"Tempo de treinamento: {train_time:.2f} segundos\")\n",
    "    print(f\"Acurácia: {acc:.4f}\")\n",
    "    print(\"Matriz de Confusão:\\n\", cm)\n",
    "    print(f\"EER: {eer:.4f}\")\n",
    "\n",
    "    model_filename = f\"{name.replace(' ', '_').replace('(', '').replace(')', '')}.pkl\"\n",
    "    with open(model_filename, \"wb\") as f:\n",
    "        pickle.dump(model, f)\n",
    "    print(f\"Modelo salvo em {model_filename}\")\n",
    "\n",
    "    metrics = {\"Model\": name, \"Training Time (sec)\": round(train_time, 2), \"Accuracy\": round(acc, 4), \"EER\": round(eer, 4)}\n",
    "    pd.DataFrame([metrics]).to_csv(csv_file, mode=\"a\", header=False, index=False)\n",
    "    print(f\"Métricas para {name} adicionadas ao arquivo {csv_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f806f8d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, confusion_matrix, det_curve, DetCurveDisplay\n",
    "# Ajustar rótulos para -1 (spoof) e +1 (bonafide), conforme exige o L2MarginLoss\n",
    "y_train_margin = np.where(y_train == 1, -1, 1)\n",
    "y_test_margin = np.where(y_test == 1, -1, 1)\n",
    "\n",
    "# Definir modelo PySR com boas práticas para classificação\n",
    "models = {\n",
    "    \"PySR (L2MarginLoss)\": PySRRegressor(\n",
    "        niterations=60,\n",
    "        binary_operators=[\"+\", \"-\", \"*\", \"/\"],\n",
    "        unary_operators=[\"exp\", \"log\", \"abs\"],\n",
    "        loss=\"L2MarginLoss()\",\n",
    "        random_state=42\n",
    "    )\n",
    "}\n",
    "\n",
    "# Mesmo CSV utilizado pelos outros modelos\n",
    "csv_file = \"model_metrics.csv\"\n",
    "\n",
    "if not os.path.exists(csv_file):\n",
    "    pd.DataFrame(columns=[\"Model\", \"Training Time (sec)\", \"Accuracy\", \"EER\"]).to_csv(csv_file, index=False)\n",
    "\n",
    "print(\"\\nTraining PySR model...\")\n",
    "\n",
    "for name, model in tqdm(models.items(), desc=\"PySR\"):\n",
    "    print(f\"\\n{name}:\")\n",
    "    start_time = time.time()\n",
    "\n",
    "    model.fit(X_train, y_train_margin)\n",
    "    train_time = time.time() - start_time\n",
    "\n",
    "    # Previsões e classificação binária por sinal\n",
    "    y_scores = model.predict(X_test)\n",
    "    y_pred_margin = np.sign(y_scores)\n",
    "    y_pred = np.where(y_pred_margin == -1, 1, 0)  # volta ao formato 0=bonafide, 1=spoof\n",
    "    y_test_bin = np.where(y_test_margin == -1, 1, 0)\n",
    "\n",
    "    acc = accuracy_score(y_test_bin, y_pred)\n",
    "    cm = confusion_matrix(y_test_bin, y_pred)\n",
    "\n",
    "    # Cálculo do EER\n",
    "    fpr, fnr, thresholds = det_curve(y_test_bin, y_scores)\n",
    "    idx = np.nanargmin(np.abs(fpr - fnr))\n",
    "    eer = fpr[idx]\n",
    "\n",
    "    print(f\"Training time: {train_time:.2f} seconds\")\n",
    "    print(f\"Accuracy: {acc:.4f}\")\n",
    "    print(\"Confusion Matrix:\\n\", cm)\n",
    "    print(f\"EER: {eer:.4f}\")\n",
    "\n",
    "    # Salvar modelo\n",
    "    model_filename = f\"{name.replace(' ', '_').replace('(', '').replace(')', '')}.pkl\"\n",
    "    with open(model_filename, \"wb\") as f:\n",
    "        pickle.dump(model, f)\n",
    "    print(f\"Model saved to {model_filename}\")\n",
    "\n",
    "    # Salvar métricas no CSV\n",
    "    metrics = {\n",
    "        \"Model\": name,\n",
    "        \"Training Time (sec)\": round(train_time, 2),\n",
    "        \"Accuracy\": round(acc, 4),\n",
    "        \"EER\": round(eer, 4)\n",
    "    }\n",
    "\n",
    "    pd.DataFrame([metrics]).to_csv(csv_file, mode=\"a\", header=False, index=False)\n",
    "    print(f\"Metrics for {name} appended to {csv_file}\")\n",
    "\n",
    "    # Curva DET (opcional)\n",
    "    DetCurveDisplay(fpr=fpr, fnr=fnr).plot()\n",
    "    plt.title(f\"DET Curve - {name}\")\n",
    "    plt.title(\"DET Curve - PySR\")\n",
    "    plt.savefig(\"det_curve_pysr.png\", dpi=300)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d57a2e61",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "audio_symbolic_regression_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
